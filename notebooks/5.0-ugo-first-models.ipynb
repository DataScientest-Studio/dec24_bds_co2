{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5890, 29)\n",
      "Index(['lib_mrq', 'lib_mod_doss', 'lib_mod', 'dscom', 'cod_cbr', 'hybride',\n",
      "       'puiss_admin_98', 'puiss_max', 'conso_urb', 'conso_exurb',\n",
      "       'conso_mixte', 'co2', 'co_typ_1', 'nox', 'ptcl', 'masse_ordma_min',\n",
      "       'masse_ordma_max', 'champ_v9', 'Carrosserie', 'gamme', 'Mp', 'Ct',\n",
      "       'W (mm)', 'At1 (mm)', 'At2 (mm)', 'ec (cm3)', 'typ_boite', 'nb_rapp',\n",
      "       'category'],\n",
      "      dtype='object')\n",
      "      lib_mrq lib_mod_doss    lib_mod                        dscom cod_cbr  \\\n",
      "0  ALFA-ROMEO          159        159         159 2.0 JTDm (170ch)      GO   \n",
      "1  ALFA-ROMEO          159        159     159 2.0 JTDm (170ch) ECO      GO   \n",
      "2  ALFA-ROMEO          159        159      159 SW 2.0 JTDm (170ch)      GO   \n",
      "3  ALFA-ROMEO          159        159  159 SW 2.0 JTDm (170ch) ECO      GO   \n",
      "4  ALFA-ROMEO  AR8C SPIDER  8C SPIDER                    8C SPIDER      ES   \n",
      "\n",
      "  hybride  puiss_admin_98  puiss_max  conso_urb  conso_exurb   ...     \\\n",
      "0     non               9      125.0        6.9          4.3   ...      \n",
      "1     non               9      125.0        6.6          4.3   ...      \n",
      "2     non               9      125.0        7.1          4.4   ...      \n",
      "3     non               9      125.0        6.7          4.4   ...      \n",
      "4     non              38      331.0       24.4         11.6   ...      \n",
      "\n",
      "       gamme                          Mp  Ct  W (mm)  At1 (mm)  At2 (mm)  \\\n",
      "0  MOY-SUPER                         NaN  M1  2703.0    1579.0    1559.0   \n",
      "1  MOY-SUPER                         NaN  M1  2703.0    1593.0    1575.0   \n",
      "2  MOY-SUPER                         NaN  M1  2703.0    1579.0    1559.0   \n",
      "3  MOY-SUPER                         NaN  M1  2703.0    1593.0    1575.0   \n",
      "4       LUXE  FIAT GROUP AUTOMOBILES SPA  M1  2645.0    1590.0    1589.0   \n",
      "\n",
      "   ec (cm3) typ_boite nb_rapp category  \n",
      "0    1956.0         M       6        C  \n",
      "1    1956.0         M       6        C  \n",
      "2    1956.0         M       6        D  \n",
      "3    1956.0         M       6        C  \n",
      "4    4691.0         M       6        G  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5890 entries, 0 to 5889\n",
      "Data columns (total 29 columns):\n",
      "lib_mrq            5890 non-null object\n",
      "lib_mod_doss       5890 non-null object\n",
      "lib_mod            5890 non-null object\n",
      "dscom              5890 non-null object\n",
      "cod_cbr            5890 non-null object\n",
      "hybride            5890 non-null object\n",
      "puiss_admin_98     5890 non-null int64\n",
      "puiss_max          5890 non-null float64\n",
      "conso_urb          5890 non-null float64\n",
      "conso_exurb        5890 non-null float64\n",
      "conso_mixte        5890 non-null float64\n",
      "co2                5890 non-null float64\n",
      "co_typ_1           5878 non-null float64\n",
      "nox                5878 non-null float64\n",
      "ptcl               4484 non-null float64\n",
      "masse_ordma_min    5890 non-null int64\n",
      "masse_ordma_max    5890 non-null int64\n",
      "champ_v9           5890 non-null object\n",
      "Carrosserie        5890 non-null object\n",
      "gamme              5890 non-null object\n",
      "Mp                 3001 non-null object\n",
      "Ct                 5890 non-null object\n",
      "W (mm)             5890 non-null float64\n",
      "At1 (mm)           5890 non-null float64\n",
      "At2 (mm)           5890 non-null float64\n",
      "ec (cm3)           5890 non-null float64\n",
      "typ_boite          5890 non-null object\n",
      "nb_rapp            5890 non-null int64\n",
      "category           5890 non-null object\n",
      "dtypes: float64(12), int64(4), object(13)\n",
      "memory usage: 1.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Processed/data_merge_v2.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Etude 1**\n",
    "\n",
    "Dans cette première étude, on va juste tester des modèles simples sur les variables conso_mixte, puiss_max et ec (cm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only \"conso_mixte\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   conso_mixte\n",
      "0     5.300000\n",
      "1     5.100000\n",
      "2     5.400000\n",
      "3     5.200000\n",
      "4    16.299999\n",
      "0    2\n",
      "1    2\n",
      "2    3\n",
      "3    2\n",
      "4    6\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"conso_mixte\"]]\n",
    "\n",
    "y = df[\"category\"].replace(to_replace=['A','B','C','D','E','F','G'],value=[0,1,2,3,4,5,6])\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 0.6782682512733447\n",
      "Logistic Regression 0.7122241086587436\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88       145\n",
      "          1       0.77      0.96      0.85       340\n",
      "          2       0.76      0.72      0.74       298\n",
      "          3       0.51      0.74      0.61       211\n",
      "          4       0.54      0.17      0.26       161\n",
      "          5       0.00      0.00      0.00        14\n",
      "          6       0.00      0.00      0.00         9\n",
      "\n",
      "avg / total       0.70      0.71      0.69      1178\n",
      "\n",
      "col_0       0    1    2    3   4\n",
      "category                        \n",
      "0         115   30    0    0   0\n",
      "1           0  325   13    2   0\n",
      "2           0   68  215   15   0\n",
      "3           0    0   54  157   0\n",
      "4           0    0    0  134  27\n",
      "5           0    0    0    0  14\n",
      "6           0    0    0    0   9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"Logistic Regression\",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression\",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8240662139219015\n",
      "0.8488964346349746\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.97      0.87       145\n",
      "          1       0.99      0.80      0.88       340\n",
      "          2       0.87      0.90      0.88       298\n",
      "          3       0.69      0.92      0.79       211\n",
      "          4       0.92      0.65      0.76       161\n",
      "          5       1.00      0.93      0.96        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.87      0.85      0.85      1178\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         141    4    0    0    0   0  0\n",
      "1          37  271   30    1    1   0  0\n",
      "2           0    0  267   31    0   0  0\n",
      "3           0    0    9  195    7   0  0\n",
      "4           0    0    0   57  104   0  0\n",
      "5           0    0    0    0    1  13  0\n",
      "6           0    0    0    0    0   0  9\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(knn.score(X_train_scaler,y_train))\n",
    "print(knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825339558573854\n",
      "0.8514431239388794\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.79      0.88       145\n",
      "          1       0.91      0.91      0.91       340\n",
      "          2       0.89      0.84      0.86       298\n",
      "          3       0.65      0.99      0.79       211\n",
      "          4       0.99      0.61      0.75       161\n",
      "          5       0.93      1.00      0.97        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.88      0.85      0.85      1178\n",
      "\n",
      "col_0       0    1    2    3   4   5  6\n",
      "category                               \n",
      "0         115   30    0    0   0   0  0\n",
      "1           0  308   30    1   1   0  0\n",
      "2           0    0  250   48   0   0  0\n",
      "3           0    0    2  209   0   0  0\n",
      "4           0    0    0   62  98   1  0\n",
      "5           0    0    0    0   0  14  0\n",
      "6           0    0    0    0   0   0  9\n"
     ]
    }
   ],
   "source": [
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(rforest.score(X_train_scaler,y_train))\n",
    "print(rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Only puiss_max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Logistic Regression train:  0.35250424448217316\n",
      "Logistic Regression test :  0.366723259762309\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.30      0.37       145\n",
      "          1       0.38      0.63      0.47       340\n",
      "          2       0.33      0.37      0.35       298\n",
      "          3       0.31      0.28      0.30       211\n",
      "          4       0.00      0.00      0.00       161\n",
      "          5       0.00      0.00      0.00        14\n",
      "          6       1.00      0.78      0.88         9\n",
      "\n",
      "avg / total       0.32      0.37      0.33      1178\n",
      "\n",
      "col_0      0    1    2   3  6\n",
      "category                     \n",
      "0         43   93    6   3  0\n",
      "1         42  213   63  22  0\n",
      "2          2  137  109  50  0\n",
      "3          0   78   73  60  0\n",
      "4          0   42   73  46  0\n",
      "5          0    0    2  12  0\n",
      "6          0    0    0   2  7\n",
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNN train :  0.47877758913412566\n",
      "KNN test :  0.5118845500848896\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.63      0.47       145\n",
      "          1       0.50      0.53      0.52       340\n",
      "          2       0.68      0.29      0.41       298\n",
      "          3       0.55      0.80      0.65       211\n",
      "          4       0.46      0.35      0.40       161\n",
      "          5       0.85      0.79      0.81        14\n",
      "          6       1.00      0.89      0.94         9\n",
      "\n",
      "avg / total       0.54      0.51      0.50      1178\n",
      "\n",
      "col_0       0    1   2    3   4   5  6\n",
      "category                              \n",
      "0          91   47   1    0   6   0  0\n",
      "1         100  180  23   14  23   0  0\n",
      "2          30   97  87   60  24   0  0\n",
      "3           4   11  11  169  15   1  0\n",
      "4          14   20   6   64  57   0  0\n",
      "5           0    2   0    1   0  11  0\n",
      "6           0    0   0    0   0   1  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "Random Forest train :  0.5418081494057725\n",
      "Random Forest test :  0.5585738539898133\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.46      0.53       145\n",
      "          1       0.53      0.66      0.59       340\n",
      "          2       0.57      0.42      0.48       298\n",
      "          3       0.61      0.68      0.64       211\n",
      "          4       0.47      0.48      0.48       161\n",
      "          5       0.85      0.79      0.81        14\n",
      "          6       1.00      0.89      0.94         9\n",
      "\n",
      "avg / total       0.56      0.56      0.55      1178\n",
      "\n",
      "col_0      0    1    2    3   4   5  6\n",
      "category                              \n",
      "0         67   69    3    0   6   0  0\n",
      "1         39  224   55    9  13   0  0\n",
      "2          1   94  126   43  34   0  0\n",
      "3          0    7   25  144  34   1  0\n",
      "4          0   31   11   41  78   0  0\n",
      "5          0    0    2    1   0  11  0\n",
      "6          0    0    0    0   0   1  8\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"puiss_max\"]]\n",
    "y = df[\"category\"].replace(to_replace=['A','B','C','D','E','F','G'],value=[0,1,2,3,4,5,6])\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "###LogisticRegression\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Logistic Regression train: \",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression test : \",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"KNN\")\n",
    "print(\"KNN train : \",knn.score(X_train_scaler,y_train))\n",
    "print(\"KNN test : \",knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_scaler,y_train))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ec (cm3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Logistic Regression train:  0.30708828522920206\n",
      "Logistic Regression test :  0.34295415959252973\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       145\n",
      "          1       0.34      0.79      0.48       340\n",
      "          2       0.30      0.31      0.30       298\n",
      "          3       0.00      0.00      0.00       211\n",
      "          4       0.42      0.22      0.29       161\n",
      "          5       0.00      0.00      0.00        14\n",
      "          6       0.89      0.89      0.89         9\n",
      "\n",
      "avg / total       0.24      0.34      0.26      1178\n",
      "\n",
      "col_0       1   2   4  6\n",
      "category                \n",
      "0         136   9   0  0\n",
      "1         269  67   4  0\n",
      "2         188  91  18  1\n",
      "3         146  50  15  0\n",
      "4          45  80  36  0\n",
      "5           0   2  12  0\n",
      "6           0   0   1  8\n",
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNN train :  0.41256366723259763\n",
      "KNN test :  0.4074702886247878\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.76      0.45       145\n",
      "          1       0.58      0.32      0.41       340\n",
      "          2       0.48      0.42      0.45       298\n",
      "          3       0.39      0.18      0.25       211\n",
      "          4       0.29      0.48      0.36       161\n",
      "          5       1.00      0.71      0.83        14\n",
      "          6       0.90      1.00      0.95         9\n",
      "\n",
      "avg / total       0.46      0.41      0.40      1178\n",
      "\n",
      "col_0       0    1    2   3    4   5  6\n",
      "category                               \n",
      "0         110   27    7   0    1   0  0\n",
      "1         161  108   42   7   22   0  0\n",
      "2          61   46  126  25   39   0  1\n",
      "3           6    4   39  39  123   0  0\n",
      "4           2    2   49  30   78   0  0\n",
      "5           0    0    0   0    4  10  0\n",
      "6           0    0    0   0    0   0  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "Random Forest train :  0.5568760611205433\n",
      "Random Forest test :  0.567062818336163\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.53      0.54       145\n",
      "          1       0.58      0.57      0.58       340\n",
      "          2       0.51      0.56      0.54       298\n",
      "          3       0.58      0.67      0.62       211\n",
      "          4       0.60      0.43      0.50       161\n",
      "          5       1.00      0.79      0.88        14\n",
      "          6       1.00      0.89      0.94         9\n",
      "\n",
      "avg / total       0.57      0.57      0.57      1178\n",
      "\n",
      "col_0      0    1    2    3   4   5  6\n",
      "category                              \n",
      "0         77   60    6    2   0   0  0\n",
      "1         54  194   65   20   7   0  0\n",
      "2          8   65  168   40  17   0  0\n",
      "3          0   10   41  141  19   0  0\n",
      "4          0    4   49   39  69   0  0\n",
      "5          0    0    0    0   3  11  0\n",
      "6          0    1    0    0   0   0  8\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"ec (cm3)\"]]\n",
    "y = df[\"category\"].replace(to_replace=['A','B','C','D','E','F','G'],value=[0,1,2,3,4,5,6])\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "###LogisticRegression\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Logistic Regression train: \",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression test : \",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"KNN\")\n",
    "print(\"KNN train : \",knn.score(X_train_scaler,y_train))\n",
    "print(\"KNN test : \",knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_scaler,y_train))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conso_mixte + puiss_max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Logistic Regression train:  0.634974533106961\n",
      "Logistic Regression test :  0.6706281833616299\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.67      0.78       145\n",
      "          1       0.75      0.94      0.84       340\n",
      "          2       0.62      0.68      0.65       298\n",
      "          3       0.48      0.46      0.47       211\n",
      "          4       0.61      0.43      0.50       161\n",
      "          5       0.00      0.00      0.00        14\n",
      "          6       1.00      0.33      0.50         9\n",
      "\n",
      "avg / total       0.67      0.67      0.66      1178\n",
      "\n",
      "col_0      0    1    2   3   4  5  6\n",
      "category                            \n",
      "0         97   48    0   0   0  0  0\n",
      "1          6  319   13   0   2  0  0\n",
      "2          0   56  204  38   0  0  0\n",
      "3          0    0   88  98  25  0  0\n",
      "4          0    0   24  68  69  0  0\n",
      "5          0    0    0   1  13  0  0\n",
      "6          0    0    0   0   5  1  3\n",
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNN train :  0.9898132427843803\n",
      "KNN test :  0.9881154499151104\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       145\n",
      "          1       0.99      1.00      0.99       340\n",
      "          2       0.99      0.98      0.98       298\n",
      "          3       0.96      1.00      0.98       211\n",
      "          4       0.99      0.98      0.99       161\n",
      "          5       1.00      0.93      0.96        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1178\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         144    1    0    0    0   0  0\n",
      "1           0  339    1    0    0   0  0\n",
      "2           0    2  291    5    0   0  0\n",
      "3           0    0    1  210    0   0  0\n",
      "4           0    0    0    3  158   0  0\n",
      "5           0    0    0    0    1  13  0\n",
      "6           0    0    0    0    0   0  9\n",
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "Random Forest train :  0.9970288624787776\n",
      "Random Forest test :  0.9940577249575552\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       145\n",
      "          1       0.99      1.00      0.99       340\n",
      "          2       1.00      0.99      0.99       298\n",
      "          3       0.99      1.00      0.99       211\n",
      "          4       1.00      0.98      0.99       161\n",
      "          5       1.00      1.00      1.00        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1178\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         144    1    0    0    0   0  0\n",
      "1           0  339    1    0    0   0  0\n",
      "2           0    2  296    0    0   0  0\n",
      "3           0    0    0  211    0   0  0\n",
      "4           0    0    0    3  158   0  0\n",
      "5           0    0    0    0    0  14  0\n",
      "6           0    0    0    0    0   0  9\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"conso_mixte\",\"puiss_max\"]]\n",
    "y = df[\"category\"].replace(to_replace=['A','B','C','D','E','F','G'],value=[0,1,2,3,4,5,6])\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "###LogisticRegression\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Logistic Regression train: \",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression test : \",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"KNN\")\n",
    "print(\"KNN train : \",knn.score(X_train_scaler,y_train))\n",
    "print(\"KNN test : \",knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_scaler,y_train))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conso_mixte + ec (cm3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Logistic Regression train:  0.6305178268251274\n",
      "Logistic Regression test :  0.6536502546689303\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.77      0.85       145\n",
      "          1       0.82      0.95      0.88       340\n",
      "          2       0.60      0.60      0.60       298\n",
      "          3       0.43      0.67      0.52       211\n",
      "          4       0.41      0.08      0.13       161\n",
      "          5       0.00      0.00      0.00        14\n",
      "          6       0.75      0.33      0.46         9\n",
      "\n",
      "avg / total       0.64      0.65      0.63      1178\n",
      "\n",
      "col_0       0    1    2    3   4  5  6\n",
      "category                              \n",
      "0         111   34    0    0   0  0  0\n",
      "1           5  323    1   11   0  0  0\n",
      "2           0   39  179   79   0  0  1\n",
      "3           0    0   70  141   0  0  0\n",
      "4           0    0   48  100  13  0  0\n",
      "5           0    0    0    0  14  0  0\n",
      "6           0    0    0    0   5  1  3\n",
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNN train :  0.9940577249575552\n",
      "KNN test :  0.9872665534804754\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       145\n",
      "          1       0.99      0.99      0.99       340\n",
      "          2       1.00      0.98      0.99       298\n",
      "          3       0.97      1.00      0.98       211\n",
      "          4       0.98      0.97      0.97       161\n",
      "          5       0.92      0.86      0.89        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1178\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         145    0    0    0    0   0  0\n",
      "1           0  338    1    1    0   0  0\n",
      "2           0    3  293    1    0   1  0\n",
      "3           0    0    0  210    1   0  0\n",
      "4           0    0    0    5  156   0  0\n",
      "5           0    0    0    0    2  12  0\n",
      "6           0    0    0    0    0   0  9\n",
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "Random Forest train :  0.9978777589134126\n",
      "Random Forest test :  0.9915110356536503\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       145\n",
      "          1       0.99      0.99      0.99       340\n",
      "          2       1.00      0.99      0.99       298\n",
      "          3       0.98      1.00      0.99       211\n",
      "          4       0.99      0.98      0.98       161\n",
      "          5       1.00      1.00      1.00        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1178\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         145    0    0    0    0   0  0\n",
      "1           0  338    1    1    0   0  0\n",
      "2           0    3  295    0    0   0  0\n",
      "3           0    0    0  210    1   0  0\n",
      "4           0    0    0    4  157   0  0\n",
      "5           0    0    0    0    0  14  0\n",
      "6           0    0    0    0    0   0  9\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"conso_mixte\",\"ec (cm3)\"]]\n",
    "y = df[\"category\"].replace(to_replace=['A','B','C','D','E','F','G'],value=[0,1,2,3,4,5,6])\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "###LogisticRegression\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Logistic Regression train: \",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression test : \",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"KNN\")\n",
    "print(\"KNN train : \",knn.score(X_train_scaler,y_train))\n",
    "print(\"KNN test : \",knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_scaler,y_train))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ec (cm3) + puiss_max**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Logistic Regression train:  0.3716044142614601\n",
      "Logistic Regression test :  0.40407470288624786\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.15      0.23       145\n",
      "          1       0.41      0.72      0.53       340\n",
      "          2       0.36      0.24      0.29       298\n",
      "          3       0.36      0.50      0.42       211\n",
      "          4       0.62      0.16      0.25       161\n",
      "          5       0.00      0.00      0.00        14\n",
      "          6       1.00      0.78      0.88         9\n",
      "\n",
      "avg / total       0.42      0.40      0.37      1178\n",
      "\n",
      "col_0      0    1   2    3   4  6\n",
      "category                         \n",
      "0         22  114   6    3   0  0\n",
      "1         21  246  49   24   0  0\n",
      "2          7  131  71   82   7  0\n",
      "3          0   78  27  105   1  0\n",
      "4          0   26  43   67  25  0\n",
      "5          0    0   0    9   5  0\n",
      "6          0    0   0    0   2  7\n",
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNN train :  0.5439303904923599\n",
      "KNN test :  0.5246179966044142\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.28      0.72      0.40       145\n",
      "          1       0.63      0.45      0.53       340\n",
      "          2       0.68      0.41      0.51       298\n",
      "          3       0.61      0.64      0.62       211\n",
      "          4       0.61      0.52      0.56       161\n",
      "          5       0.92      0.79      0.85        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.60      0.52      0.54      1178\n",
      "\n",
      "col_0       0    1    2    3   4   5  6\n",
      "category                               \n",
      "0         104   41    0    0   0   0  0\n",
      "1         149  154   31    6   0   0  0\n",
      "2          71   47  123   45  11   1  0\n",
      "3          22    1   14  134  40   0  0\n",
      "4          30    2   13   33  83   0  0\n",
      "5           0    0    0    0   3  11  0\n",
      "6           0    0    0    0   0   0  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "Random Forest train :  0.6362478777589134\n",
      "Random Forest test :  0.6273344651952462\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.49      0.56       145\n",
      "          1       0.61      0.73      0.66       340\n",
      "          2       0.65      0.45      0.53       298\n",
      "          3       0.66      0.73      0.69       211\n",
      "          4       0.55      0.67      0.61       161\n",
      "          5       1.00      0.93      0.96        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.63      0.63      0.62      1178\n",
      "\n",
      "col_0      0    1    2    3    4   5  6\n",
      "category                               \n",
      "0         71   67    1    0    6   0  0\n",
      "1         34  249   42    2   13   0  0\n",
      "2          3   87  135   37   36   0  0\n",
      "3          0    6   20  154   31   0  0\n",
      "4          0    2    9   42  108   0  0\n",
      "5          0    0    0    0    1  13  0\n",
      "6          0    0    0    0    0   0  9\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"ec (cm3)\",\"puiss_max\"]]\n",
    "y = df[\"category\"].replace(to_replace=['A','B','C','D','E','F','G'],value=[0,1,2,3,4,5,6])\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "###LogisticRegression\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Logistic Regression train: \",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression test : \",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"KNN\")\n",
    "print(\"KNN train : \",knn.score(X_train_scaler,y_train))\n",
    "print(\"KNN test : \",knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_scaler,y_train))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**conso_mixte + puiss_max + ec (cm3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Logistic Regression train:  0.6515280135823429\n",
      "Logistic Regression test :  0.6621392190152802\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.69      0.80       145\n",
      "          1       0.79      0.94      0.86       340\n",
      "          2       0.59      0.55      0.57       298\n",
      "          3       0.47      0.64      0.54       211\n",
      "          4       0.63      0.35      0.45       161\n",
      "          5       0.00      0.00      0.00        14\n",
      "          6       1.00      0.33      0.50         9\n",
      "\n",
      "avg / total       0.67      0.66      0.65      1178\n",
      "\n",
      "col_0       0    1    2    3   4  5  6\n",
      "category                              \n",
      "0         100   45    0    0   0  0  0\n",
      "1           6  321    3    8   2  0  0\n",
      "2           0   42  165   88   3  0  0\n",
      "3           0    0   67  134  10  0  0\n",
      "4           0    0   47   57  57  0  0\n",
      "5           0    0    0    1  13  0  0\n",
      "6           0    0    0    0   5  1  3\n",
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNN train :  0.9927843803056027\n",
      "KNN test :  0.9881154499151104\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99       145\n",
      "          1       0.98      0.99      0.99       340\n",
      "          2       1.00      0.99      0.99       298\n",
      "          3       0.98      1.00      0.99       211\n",
      "          4       0.99      0.98      0.98       161\n",
      "          5       1.00      0.93      0.96        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1178\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         143    2    0    0    0   0  0\n",
      "1           0  338    1    1    0   0  0\n",
      "2           0    4  294    0    0   0  0\n",
      "3           0    0    0  210    1   0  0\n",
      "4           0    0    0    4  157   0  0\n",
      "5           0    0    0    0    1  13  0\n",
      "6           0    0    0    0    0   0  9\n",
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "Random Forest train :  0.9995755517826825\n",
      "Random Forest test :  0.9906621392190152\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       145\n",
      "          1       1.00      1.00      1.00       340\n",
      "          2       1.00      0.99      0.99       298\n",
      "          3       0.97      1.00      0.98       211\n",
      "          4       0.99      0.97      0.98       161\n",
      "          5       1.00      1.00      1.00        14\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1178\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         144    1    0    0    0   0  0\n",
      "1           0  339    1    0    0   0  0\n",
      "2           1    0  295    1    1   0  0\n",
      "3           0    0    0  210    1   0  0\n",
      "4           0    0    0    5  156   0  0\n",
      "5           0    0    0    0    0  14  0\n",
      "6           0    0    0    0    0   0  9\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"conso_mixte\",\"puiss_max\",\"ec (cm3)\"]]\n",
    "y = df[\"category\"].replace(to_replace=['A','B','C','D','E','F','G'],value=[0,1,2,3,4,5,6])\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "###LogisticRegression\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Logistic Regression train: \",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression test : \",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"KNN\")\n",
    "print(\"KNN train : \",knn.score(X_train_scaler,y_train))\n",
    "print(\"KNN test : \",knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_scaler,y_train))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion de l'étude 1**\n",
    "\n",
    "\"conso_mixte\" est très utile et permet une classification très efficace. Si on combine cette donnée à \"puiss_max\" ou \"ec (cm3)\", on obtient 99% d'accuracy.\n",
    "La question que l'on peut se poser est : \"Est ce que la conso_mixte n'est pas trop corrélée avec la variable cible et qu'il est donc pas pertinent de faire un modèle à partir de cette variable ?\". En effet, on obtient déjà 99% d'accuracy sans essayer de perfectionner le modèle. Cependant pour nuancer cela, il faut quand même prendre en compte que en utilisant uniquement la variable, on obtient 85%. C'est donc grâce à la combinaison de cette variable avec d'autres que l'on obtient des scores élevés.\n",
    "\n",
    "De plus, j'ai testé \"conso_urb\" et \"conso_exurb\" à la place de \"conso_mixte\" par curiosité. C'est variable sont également très efficaces mais moins que \"conso_mixte\". Par exemple, on pert environ 5 à 7 % d'accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lib_mrq', 'lib_mod_doss', 'lib_mod', 'dscom', 'cod_cbr', 'hybride',\n",
       "       'puiss_admin_98', 'puiss_max', 'conso_urb', 'conso_exurb',\n",
       "       'conso_mixte', 'co2', 'co_typ_1', 'nox', 'ptcl', 'masse_ordma_min',\n",
       "       'masse_ordma_max', 'champ_v9', 'Carrosserie', 'gamme', 'Mp', 'Ct',\n",
       "       'W (mm)', 'At1 (mm)', 'At2 (mm)', 'ec (cm3)', 'typ_boite', 'nb_rapp',\n",
       "       'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Etude 2 **\n",
    "\n",
    "La seconde étude utilisent uniquement des variables qui sont des caractéristiques de construction de la voiture. Par exemple la masse de la voiture, ou le type de carrosserie. L'objectif est de faire une étude en se basant sur des variables qui sont connues avant de faire des tests sur la voiture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['cod_cbr','hybride','masse_ordma_min','masse_ordma_max',\"puiss_max\",\"W (mm)\",\"At1 (mm)\",\"At2 (mm)\",'Carrosserie','typ_boite','nb_rapp','category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df2[\"hybride\"] = df2[\"hybride\"].replace(to_replace=[\"non\",\"oui\"],value=[0,1])\n",
    "df2[\"category\"] = df2[\"category\"].replace(to_replace=['A','B','C','D','E','F','G'],value=[0,1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO       3578\n",
      "ES       2239\n",
      "GH         57\n",
      "GP/ES       7\n",
      "ES/GP       6\n",
      "GN/ES       3\n",
      "Name: cod_cbr, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df2[\"cod_cbr\"].value_counts())\n",
    "df2 = df2.loc[(df2[\"cod_cbr\"] == \"GO\")| (df2[\"cod_cbr\"] == \"ES\") | (df2[\"cod_cbr\"] == \"GH\")].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hybride', 'masse_ordma_min', 'masse_ordma_max', 'puiss_max', 'W (mm)',\n",
      "       'At1 (mm)', 'At2 (mm)', 'nb_rapp', 'category', 'cod_cbr_ES',\n",
      "       'cod_cbr_GH', 'cod_cbr_GO', 'Carrosserie_BERLINE', 'Carrosserie_BREAK',\n",
      "       'Carrosserie_CABRIOLET', 'Carrosserie_COMBISPACE', 'Carrosserie_COUPE',\n",
      "       'Carrosserie_MINIBUS', 'Carrosserie_MINISPACE', 'Carrosserie_MONOSPACE',\n",
      "       'Carrosserie_MONOSPACE COMPACT', 'Carrosserie_TS TERRAINS/CHEMINS',\n",
      "       'typ_boite_A', 'typ_boite_D', 'typ_boite_M'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.get_dummies(df2)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNN train :  0.9163651840817195\n",
      "KNN test :  0.8774468085106383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.90      0.90       157\n",
      "          1       0.88      0.88      0.88       330\n",
      "          2       0.83      0.87      0.85       295\n",
      "          3       0.86      0.88      0.87       215\n",
      "          4       0.94      0.90      0.92       149\n",
      "          5       0.93      0.65      0.76        20\n",
      "          6       1.00      0.78      0.88         9\n",
      "\n",
      "avg / total       0.88      0.88      0.88      1175\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         141   13    3    0    0   0  0\n",
      "1          13  289   25    3    0   0  0\n",
      "2           1   20  257   16    1   0  0\n",
      "3           0    3   20  190    2   0  0\n",
      "4           0    2    3   10  134   0  0\n",
      "5           0    0    0    2    5  13  0\n",
      "6           0    0    0    0    1   1  7\n",
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "Random Forest train :  0.9831879123217706\n",
      "Random Forest test :  0.9438297872340425\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.97      0.95       157\n",
      "          1       0.93      0.94      0.94       330\n",
      "          2       0.94      0.92      0.93       295\n",
      "          3       0.94      0.95      0.95       215\n",
      "          4       0.99      0.96      0.98       149\n",
      "          5       1.00      0.95      0.97        20\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1175\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         152    4    1    0    0   0  0\n",
      "1          11  311    7    1    0   0  0\n",
      "2           0   17  270    8    0   0  0\n",
      "3           0    2    7  205    1   0  0\n",
      "4           0    1    2    3  143   0  0\n",
      "5           0    0    0    1    0  19  0\n",
      "6           0    0    0    0    0   0  9\n",
      "\n",
      "\n",
      "\n",
      "                            Feature  Importance\n",
      "3                         puiss_max    0.183696\n",
      "2                   masse_ordma_max    0.182722\n",
      "1                   masse_ordma_min    0.173144\n",
      "4                            W (mm)    0.089567\n",
      "6                          At2 (mm)    0.060438\n",
      "5                          At1 (mm)    0.051194\n",
      "8                        cod_cbr_ES    0.045248\n",
      "7                           nb_rapp    0.033638\n",
      "10                       cod_cbr_GO    0.030507\n",
      "21                      typ_boite_A    0.025604\n",
      "11              Carrosserie_BERLINE    0.023305\n",
      "23                      typ_boite_M    0.022154\n",
      "14           Carrosserie_COMBISPACE    0.017676\n",
      "20  Carrosserie_TS TERRAINS/CHEMINS    0.012664\n",
      "16              Carrosserie_MINIBUS    0.010712\n",
      "12                Carrosserie_BREAK    0.009444\n",
      "19    Carrosserie_MONOSPACE COMPACT    0.005948\n",
      "18            Carrosserie_MONOSPACE    0.005399\n",
      "15                Carrosserie_COUPE    0.005044\n",
      "9                        cod_cbr_GH    0.004003\n",
      "13            Carrosserie_CABRIOLET    0.003575\n",
      "17            Carrosserie_MINISPACE    0.002249\n",
      "0                           hybride    0.001532\n",
      "22                      typ_boite_D    0.000535\n"
     ]
    }
   ],
   "source": [
    "#'cod_cbr','Carrosserie',typ_boite\n",
    "X = df2.drop([\"category\"],axis=1)\n",
    "y = df2[\"category\"]\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "###LogisticRegression\n",
    "'''\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Logistic Regression train: \",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression test : \",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "'''\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"KNN\")\n",
    "print(\"KNN train : \",knn.score(X_train_scaler,y_train))\n",
    "print(\"KNN test : \",knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_scaler,y_train))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "\n",
    "importances = rforest.feature_importances_\n",
    "\n",
    "features = X.columns  \n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\n\\n\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le feature importance permet montrer quelles sont les variables les plus utiles pour l'apprentissage du modèle. Ainsi, juste pour tester, j'ai selectionné les variables les plus importantes et je me suis débarrassé des autres pour voir si le modèle était plus performant en réduisant sa complexité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "KNN\n",
      "KNN train :  0.8938071930197914\n",
      "KNN test :  0.8170212765957446\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.83      0.83       157\n",
      "          1       0.85      0.83      0.84       330\n",
      "          2       0.80      0.82      0.81       295\n",
      "          3       0.82      0.80      0.81       215\n",
      "          4       0.76      0.79      0.77       149\n",
      "          5       0.94      0.80      0.86        20\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.82      0.82      0.82      1175\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         130   23    3    1    0   0  0\n",
      "1          23  273   29    3    2   0  0\n",
      "2           3   21  242   16   13   0  0\n",
      "3           0    6   18  172   19   0  0\n",
      "4           1    0   12   17  118   1  0\n",
      "5           0    0    0    0    4  16  0\n",
      "6           0    0    0    0    0   0  9\n",
      "\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "Random Forest train :  0.9789316875931049\n",
      "Random Forest test :  0.934468085106383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.96      0.93       157\n",
      "          1       0.92      0.93      0.93       330\n",
      "          2       0.95      0.91      0.93       295\n",
      "          3       0.94      0.95      0.94       215\n",
      "          4       0.95      0.95      0.95       149\n",
      "          5       0.89      0.85      0.87        20\n",
      "          6       1.00      1.00      1.00         9\n",
      "\n",
      "avg / total       0.93      0.93      0.93      1175\n",
      "\n",
      "col_0       0    1    2    3    4   5  6\n",
      "category                                \n",
      "0         150    6    1    0    0   0  0\n",
      "1          14  308    7    1    0   0  0\n",
      "2           0   18  268    9    0   0  0\n",
      "3           0    1    6  204    4   0  0\n",
      "4           0    2    0    3  142   2  0\n",
      "5           0    0    0    0    3  17  0\n",
      "6           0    0    0    0    0   0  9\n"
     ]
    }
   ],
   "source": [
    "#'cod_cbr','Carrosserie',typ_boite\n",
    "#X = df[['masse_ordma_min','masse_ordma_max',\"puiss_max\",\"W (mm)\",\"At1 (mm)\",\"At2 (mm)\",'nb_rapp']]\n",
    "X = df2[['masse_ordma_min','masse_ordma_max',\"puiss_max\",\"W (mm)\",\"At1 (mm)\",\"At2 (mm)\",'cod_cbr_ES','cod_cbr_GO',\"nb_rapp\"]]\n",
    "y = df2[\"category\"]\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "###LogisticRegression\n",
    "'''\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Logistic Regression train: \",clf.score(X_train_scaler,y_train))\n",
    "print(\"Logistic Regression test : \",clf.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = clf.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "'''\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"KNN\")\n",
    "print(\"KNN train : \",knn.score(X_train_scaler,y_train))\n",
    "print(\"KNN test : \",knn.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_scaler,y_train))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle n'est finalement pas plus performant. Mais j'étais curieux de voir les résultats. Ainsi je ne vais pas me débarrasser des varibles soit disant moins importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donné que le RandomForest est le modèle le plus performant utilisé, on va conserver ce modèle et essayer d'améliorer au maximum ce modèle. Pour cela, on va faire un GridSearchCV pour sélectionner la meilleur combinaison d'hyperparamètres.\n",
    "\n",
    "Les hyperparamètres testés seront le nombres d'estimateurs et le criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parametres = {\n",
    "    'n_estimators':[50,100,150,250,500]\n",
    "}\n",
    "parametres = {\n",
    "    'n_estimators':[10,50,100,150,200,300,500,750,1000],\n",
    "    'criterion':[\"gini\",\"entropy\"],\n",
    "}\n",
    "\n",
    "rforest = RandomForestClassifier(n_jobs=-1,random_state=64)\n",
    "grid_clf = GridSearchCV(estimator=rforest,param_grid = parametres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            params  mean_test_score\n",
      "0        {'criterion': 'gini', 'n_estimators': 10}         0.933390\n",
      "1        {'criterion': 'gini', 'n_estimators': 50}         0.938710\n",
      "2       {'criterion': 'gini', 'n_estimators': 100}         0.940838\n",
      "3       {'criterion': 'gini', 'n_estimators': 150}         0.939349\n",
      "4       {'criterion': 'gini', 'n_estimators': 200}         0.939349\n",
      "5       {'criterion': 'gini', 'n_estimators': 300}         0.938710\n",
      "6       {'criterion': 'gini', 'n_estimators': 500}         0.939987\n",
      "7       {'criterion': 'gini', 'n_estimators': 750}         0.940626\n",
      "8      {'criterion': 'gini', 'n_estimators': 1000}         0.939774\n",
      "9     {'criterion': 'entropy', 'n_estimators': 10}         0.933816\n",
      "10    {'criterion': 'entropy', 'n_estimators': 50}         0.941051\n",
      "11   {'criterion': 'entropy', 'n_estimators': 100}         0.939349\n",
      "12   {'criterion': 'entropy', 'n_estimators': 150}         0.939136\n",
      "13   {'criterion': 'entropy', 'n_estimators': 200}         0.939774\n",
      "14   {'criterion': 'entropy', 'n_estimators': 300}         0.938498\n",
      "15   {'criterion': 'entropy', 'n_estimators': 500}         0.939136\n",
      "16   {'criterion': 'entropy', 'n_estimators': 750}         0.939562\n",
      "17  {'criterion': 'entropy', 'n_estimators': 1000}         0.939562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\UGO\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#'cod_cbr','Carrosserie',typ_boite\n",
    "X = df2.drop([\"category\"],axis=1)\n",
    "y = df2[\"category\"]\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "grille = grid_clf.fit(X_train_scaler,y_train)\n",
    "\n",
    "print(pd.DataFrame.from_dict(grille.cv_results_).loc[:,['params','mean_test_score']])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Avec le GridSearchCV, les résultats sont pratiquement identiques quelque soit la combinaison d'hyperparamètres. Ainsi je conserve mes hyperparamètres initiaux (n_estimators = 100, criterion = 'entropy').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1571\n",
      "2    1441\n",
      "3    1154\n",
      "0     783\n",
      "4     780\n",
      "5     104\n",
      "6      41\n",
      "Name: category, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xae4cfd0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "print(df2[\"category\"].value_counts())\n",
    "\n",
    "sns.countplot(df2[\"category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donné que la répartition des classes n'est pas égale, il y a beaucoup moins d'éléments des classes 5 et 6 que les autres, j'essaye d'utiliser un resampling pour avoir un dataset plus balancé. Cette méthode permettra de diminuer l'oversampling présent notre modèle d'apprentissage. \n",
    "\n",
    "Pour cela on va utiliser la méthode de resampling nommée SMOTE. Comme pour l'instant je n'ai pas accès à la bibliothèque imblearn (il faut que je l'installe), j'ai mis en commentaire cette partie que je ferai plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from imblearn.over_sampling import SMOTE\\n\\n\\n\\n#\\'cod_cbr\\',\\'Carrosserie\\',typ_boite\\nX = df2.drop([\"category\"],axis=1)\\ny = df2[\"category\"]\\n\\n\\n#Split\\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\\n\\n#Scaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)\\nX_train_scaler = scaler.transform(X_train)\\nX_test_scaler = scaler.transform(X_test)\\n\\n#Smote\\nsmote = SMOTE()\\nX_train_smote, y_train_smote = smote.fit_resample(X_train_scaler, y_train)\\n\\n\\n#RandomForest\\nrforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\\nrforest.fit(X_train_smote,y_train_smote)\\n\\nprint(\"\\n\\n\")\\nprint(\"RANDOM FOREST\")\\nprint(\"Random Forest train : \",rforest.score(X_train_smote,y_train_smote))\\nprint(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\\n\\ny_pred = rforest.predict(X_test_scaler)\\nprint(classification_report(y_test, y_pred))\\nprint(pd.crosstab(y_test, y_pred))\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "#'cod_cbr','Carrosserie',typ_boite\n",
    "X = df2.drop([\"category\"],axis=1)\n",
    "y = df2[\"category\"]\n",
    "\n",
    "\n",
    "#Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=64)\n",
    "\n",
    "#Scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaler = scaler.transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)\n",
    "\n",
    "#Smote\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaler, y_train)\n",
    "\n",
    "\n",
    "#RandomForest\n",
    "rforest = RandomForestClassifier(n_jobs = -1,random_state = 64)\n",
    "rforest.fit(X_train_smote,y_train_smote)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"Random Forest train : \",rforest.score(X_train_smote,y_train_smote))\n",
    "print(\"Random Forest test : \",rforest.score(X_test_scaler,y_test))\n",
    "\n",
    "y_pred = rforest.predict(X_test_scaler)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(pd.crosstab(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'aimerai bien tester un modèle de Deep Learning pour voir si j'obtiens un meilleur modèle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
